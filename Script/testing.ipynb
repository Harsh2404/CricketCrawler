{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = \"../ScrapedData\" #Location where Scraped Data will be stored\n",
    "driver_path = \"../Driver/chromedriver.exe\" #Location of WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webdriver Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    driver.close()\n",
    "    print('Webdriver Loaded Successfully')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     urldf = pd.read_csv('CityUrls.csv')\n",
    "#     print('URL file Loaded')\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "# toscrape = urldf[urldf.SCRAPE == 'Y'].reset_index(drop = True)\n",
    "\n",
    "url='https://cricheroes.in/past-matches/1/Ahmedabad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected unindent (<ipython-input-34-3a632a0d1228>, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-34-3a632a0d1228>\"\u001b[1;36m, line \u001b[1;32m46\u001b[0m\n\u001b[1;33m    return 0\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected unindent\n"
     ]
    }
   ],
   "source": [
    "## STEP 5 ##\n",
    "\n",
    "def ScrapeCityData(city, url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(driver_path)\n",
    "        driver.get(url)\n",
    "        time.sleep(10)\n",
    "        print('\\nScraping Data of :',city)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "        \n",
    "#     ##### Load More Loop #####\n",
    "#     LoadMoreid = \"loadMoreMatches\"\n",
    "#     val = \" \"\n",
    "#     print('Loading Data...')\n",
    "#     while(val!='display: none;'):\n",
    "#         try:\n",
    "#             #waitForLoad(LOAD_MORE_BUTTON_XPATH)\n",
    "#             #WebDriverWait(driver, PTIME)\n",
    "#             loadMoreButton = driver.find_element_by_id(LoadMoreid)\n",
    "#             val = loadMoreButton.get_attribute('style')\n",
    "#             print(val)\n",
    "#             driver.execute_script(\"arguments[0].click();\", loadMoreButton)\n",
    "#             print('Button Clicked')\n",
    "#             time.sleep(5)\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             break\n",
    "#     print(\"Loading data Complete\")\n",
    "\n",
    "\n",
    "    res = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(res)\n",
    "    box = soup.find_all('div',{'class':'row matchesDiv'})\n",
    "    \n",
    "    # Extract Data From the HTML Source page\n",
    "    AllMatches = []\n",
    "    for matches in box:\n",
    "        for match in matches:\n",
    "            try:\n",
    "                MatchDetails = match.find('div',{'class':'pmd-card pmd-card-media-inline pmd-card-default pmd-z-depth'})\n",
    "                print(MatchDetails)\n",
    "#               info=match.find('div',{'class':'pmd-card-title-text test-match-card-title nohover'})\n",
    "#             except:\n",
    "#                 MatchDetails = None\n",
    "#             if(MatchDetails is None):\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 try:\n",
    "                \n",
    "#                 except:\n",
    "#                     matchName = 'None'\n",
    "#                 try:\n",
    "#                     Address = MatchDetails.contents[3].contents[3].contents[1].contents[0]\n",
    "#                     print(Address)\n",
    "#                 except:\n",
    "#                     Address = 'None'\n",
    "#                 try:\n",
    "#                     teamA = MatchDetails.contents[5].contents[1].contents[1].contents[1].contents[1].contents[0]\n",
    "#                 except:\n",
    "#                     teamA = 'None'\n",
    "#                 try:    \n",
    "#                     scoreA = MatchDetails.contents[5].contents[1].contents[1].contents[3].contents[1].contents[0].contents[0]\n",
    "#                 except:\n",
    "#                     scoreA = 'None'\n",
    "#                 try:\n",
    "#                     teamB = MatchDetails.contents[5].contents[3].contents[1].contents[1].contents[1].contents[0]\n",
    "#                 except:\n",
    "#                     teamB = 'None'\n",
    "#                 try:\n",
    "#                     scoreB = MatchDetails.contents[5].contents[3].contents[1].contents[3].contents[1].contents[0].contents[0]\n",
    "#                 except:\n",
    "#                     scoreB = 'None'\n",
    "#                 try:\n",
    "#                     winner = MatchDetails.contents[7].contents[1].contents[0].contents[0]\n",
    "#                 except:\n",
    "#                     winner = 'None'\n",
    "#                 try:\n",
    "#                     wonBy = MatchDetails.contents[7].contents[1].contents[2].contents[0]\n",
    "#                 except:\n",
    "#                     wonBy = 'None'\n",
    "#                 try:\n",
    "#                     matchLink = \"https://cricheroes.in\" + match.contents[1].get('href')\n",
    "#                 except:\n",
    "#                     matchLink = 'None'\n",
    "#                 oneMatch = [matchName, Address, teamA, scoreA, teamB, scoreB, winner, wonBy, matchLink]\n",
    "#                 oneMatch = [matchName]\n",
    "#                 AllMatches.append(oneMatch)\n",
    "#     print(AllMatches)\n",
    "    \n",
    "    #Saving File\n",
    "#     try:\n",
    "#         mdf = pd.DataFrame(AllMatches, columns = ['MatchName', 'Address', 'TeamA', 'ScoreA', 'TeamB', 'ScoreB', 'Winner','WonBy', 'MatchLink'])\n",
    "#         mdf.to_csv(os.path.join(result_path,city+'.csv'), index = False)\n",
    "#         print('Data of',city,' Extracted and File Saved Successfully\\n\\n')\n",
    "#         return 1\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeCityData(city, url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(driver_path)\n",
    "        driver.get(url)\n",
    "        time.sleep(10)\n",
    "        print('\\nScraping Data of :',city)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "    res = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(res)\n",
    "    box = soup.find_all('div',{'class':'row matchesDiv'})\n",
    "    \n",
    "    # Extract Data From the HTML Source page\n",
    "    AllMatches = []\n",
    "    for matches in box:\n",
    "        for match in matches:\n",
    "#             MatchDetails = match.find('div',{'class':'pmd-card pmd-card-media-inline pmd-card-default pmd-z-depth'})\n",
    "#             print(MatchDetails)\n",
    "            try:\n",
    "                address=match.find('h3',{'class':'pmd-card-title-text test-match-card-title nohover'}).text.strip()\n",
    "                address_split=address.split(\", \")\n",
    "            except:\n",
    "                continue;\n",
    "                \n",
    "            print(address_split[0])\n",
    "            print(address_split[1])\n",
    "            print(address_split[2].find())\n",
    "            print(address_split[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping Data of : Ahmedabad\n",
      "Suramya Farm Cricket Ground\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "20 Ov.\n",
      "GMDC Cricket Ground\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "5 Ov.\n",
      "GMDC Cricket Ground\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "5 Ov.\n",
      "GMDC Cricket Ground\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "5 Ov.\n",
      "SGVP Cricket Ground\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "10 Ov.\n",
      "Mahesh Cricket Ground\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "8 Ov.\n",
      "My Cricket Village\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "10 Ov.\n",
      "My Cricket Village\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "10 Ov.\n",
      "Ahmedabad University\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "10 Ov.\n",
      "My Cricket Village\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "10 Ov.\n",
      "Kankariya Railway Ground\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "20 Ov.\n",
      "Chharodi Fatak\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "12 Ov.\n",
      "opp krishna shalby hospital \n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "6 Ov.\n",
      "My Cricket Village\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "10 Ov.\n",
      "Chharodi Fatak\n",
      "Ahmedabad\n",
      "03-Feb-21\n",
      "12 Ov.\n",
      "Error Occoured: None\n"
     ]
    }
   ],
   "source": [
    "# for i in range(toscrape.shape[0]):\n",
    "#     city = toscrape.CITY.iloc[i].replace(' ','_')\n",
    "#     url = toscrape.URL.iloc[i]\n",
    "#     #print('Scraping Data of', city)\n",
    "city='Ahmedabad'\n",
    "status = ScrapeCityData(city, url)\n",
    "if(status == 0):\n",
    "    print('Data Scraping Unsucessful of City :', city)\n",
    "elif(status == 1):\n",
    "    print('Data Scraping Sucessful of City :', city)\n",
    "else:\n",
    "    print('Error Occoured:',status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harsh\n",
      "Shah\n"
     ]
    }
   ],
   "source": [
    "print(\"harsh\"+\"\\nShah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
