{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Past Match Data Mining Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run this Notebook step wise only, the steps are numbered on top of each cell\n",
    "- before running the script update the CityUrls csv file where fill  'Y'  in the cities whose data is to be scraped and  'N'  to skip that city\n",
    "- After Updating the file running STEP 4 is a must And Save file as <b>.csv</b> only\n",
    "- Scraped data will be saved in the ScrapedData Folder\n",
    "- The Function in STEP 6 will inform if the Mining of that city's data was successful or not if not run the script again after updating the url file where mark 'N' where mining was sucessful  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 1 ##\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 2 ##\n",
    "\n",
    "result_path = \"../ScrapedData\" #Location where Scraped Data will be stored\n",
    "driver_path = \"../Driver/chromedriver.exe\" #Location of WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webdriver Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "## STEP 3 ##\n",
    "\n",
    "#Importing Web Driver\n",
    "#!! Important Note !!#\n",
    "# The Chrome Driver Version Should Match the current Version of Google chrome currently installed\n",
    "# Check Current Version by Options -> Help -> About Chrome\n",
    "#Current Webdriver Version = 81\n",
    "#Download Updated version if Necessary from --> https://chromedriver.chromium.org/downloads\n",
    "\n",
    "try:\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    driver.close()\n",
    "    print('Webdriver Loaded Successfully')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL file Loaded\n"
     ]
    }
   ],
   "source": [
    "## STEP 4 ##\n",
    "\n",
    "#Importing URL file\n",
    "try:\n",
    "    urldf = pd.read_csv('CityUrls.csv')\n",
    "    print('URL file Loaded')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "toscrape = urldf[urldf.SCRAPE == 'Y'].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 5 ##\n",
    "\n",
    "def ScrapeCityData(city, url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(driver_path)\n",
    "        driver.get(url)\n",
    "        time.sleep(10)\n",
    "        print('\\nScraping Data of :',city)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "        \n",
    "    ##### Load More Loop #####\n",
    "    LoadMoreid = \"loadMoreMatches\"\n",
    "    val = \" \"\n",
    "    print('Loading Data...')\n",
    "    while(val!='display: none;'):\n",
    "        try:\n",
    "            #waitForLoad(LOAD_MORE_BUTTON_XPATH)\n",
    "            #WebDriverWait(driver, PTIME)\n",
    "            loadMoreButton = driver.find_element_by_id(LoadMoreid)\n",
    "            val = loadMoreButton.get_attribute('style')\n",
    "            print(val)\n",
    "            driver.execute_script(\"arguments[0].click();\", loadMoreButton)\n",
    "            print('Button Clicked')\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    print(\"Loading data Complete\")\n",
    "\n",
    "\n",
    "    res = driver.execute_script(\"return document.documentElement.outerHTML\")\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(res)\n",
    "    box = soup.find_all('div',{'class':'row matchesDiv'})\n",
    "    \n",
    "    # Extract Data From the HTML Source page\n",
    "    AllMatches = []\n",
    "    for matches in box:\n",
    "        for match in matches:\n",
    "            try:\n",
    "                MatchDetails = match.find('div',{'class':'pmd-card pmd-card-media-inline pmd-card-default pmd-z-depth'})\n",
    "                \n",
    "                MatchDetails = None\n",
    "            if(MatchDetails is None):\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    matchName = MatchDetails.contents[1].contents[1].contents[0].contents[0]  \n",
    "                except:\n",
    "                    matchName = 'None'\n",
    "                try:\n",
    "                    Address = MatchDetails.contents[3].contents[3].contents[1].contents[0]\n",
    "                    print(Address)\n",
    "                except:\n",
    "                    Address = 'None'\n",
    "                try:\n",
    "                    teamA = MatchDetails.contents[5].contents[1].contents[1].contents[1].contents[1].contents[0]\n",
    "                except:\n",
    "                    teamA = 'None'\n",
    "                try:    \n",
    "                    scoreA = MatchDetails.contents[5].contents[1].contents[1].contents[3].contents[1].contents[0].contents[0]\n",
    "                except:\n",
    "                    scoreA = 'None'\n",
    "                try:\n",
    "                    teamB = MatchDetails.contents[5].contents[3].contents[1].contents[1].contents[1].contents[0]\n",
    "                except:\n",
    "                    teamB = 'None'\n",
    "                try:\n",
    "                    scoreB = MatchDetails.contents[5].contents[3].contents[1].contents[3].contents[1].contents[0].contents[0]\n",
    "                except:\n",
    "                    scoreB = 'None'\n",
    "                try:\n",
    "                    winner = MatchDetails.contents[7].contents[1].contents[0].contents[0]\n",
    "                except:\n",
    "                    winner = 'None'\n",
    "                try:\n",
    "                    wonBy = MatchDetails.contents[7].contents[1].contents[2].contents[0]\n",
    "                except:\n",
    "                    wonBy = 'None'\n",
    "                try:\n",
    "                    matchLink = \"https://cricheroes.in\" + match.contents[1].get('href')\n",
    "                except:\n",
    "                    matchLink = 'None'\n",
    "                oneMatch = [matchName, Address, teamA, scoreA, teamB, scoreB, winner, wonBy, matchLink]\n",
    "                AllMatches.append(oneMatch)\n",
    "    \n",
    "    #Saving File\n",
    "    try:\n",
    "        mdf = pd.DataFrame(AllMatches, columns = ['MatchName', 'Address', 'TeamA', 'ScoreA', 'TeamB', 'ScoreB', 'Winner','WonBy', 'MatchLink'])\n",
    "        mdf.to_csv(os.path.join(result_path,city+'.csv'), index = False)\n",
    "        print('Data of',city,' Extracted and File Saved Successfully\\n\\n')\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping Data of : Ahmedabad\n",
      "Loading Data...\n",
      "\n",
      "Button Clicked\n",
      "\n",
      "Button Clicked\n",
      "\n",
      "Button Clicked\n",
      "\n",
      "Button Clicked\n",
      "\n",
      "Button Clicked\n",
      "\n",
      "Button Clicked\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-991f5928fb38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoscrape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print('Scraping Data of', city)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScrapeCityData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data Scraping Unsucessful of City :'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-f2a09cfb5c43>\u001b[0m in \u001b[0;36mScrapeCityData\u001b[1;34m(city, url)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arguments[0].click();\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloadMoreButton\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Button Clicked'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## STEP 6 ##\n",
    "\n",
    "for i in range(toscrape.shape[0]):\n",
    "    city = toscrape.CITY.iloc[i].replace(' ','_')\n",
    "    url = toscrape.URL.iloc[i]\n",
    "    #print('Scraping Data of', city)\n",
    "    status = ScrapeCityData(city, url)\n",
    "    if(status == 0):\n",
    "        print('Data Scraping Unsucessful of City :', city)\n",
    "    elif(status == 1):\n",
    "        print('Data Scraping Sucessful of City :', city)\n",
    "    else:\n",
    "        print('Error Occoured:',status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
